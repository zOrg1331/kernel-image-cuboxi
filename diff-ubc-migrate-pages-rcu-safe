Subject: [PATCH rh5] ub: ub_migrate_pb remove old pb and add new pb instead of moving old bp
From: Andrey Vagin <avagin@openvz.org>
Date: Mon, 14 Feb 2011 20:25:21 +0300
To: Pavel Emelianov <xemul@parallels.com>
CC: vzlin-dev <vzlin-dev@parallels.com>, Andrey Vagin <avagin@parallels.com>
Message-ID: <1297704321-14631-1-git-send-email-avagin@openvz.org>

Now page_beancounters live in hash under RCU, so we can't change
pb->next_hash, we can remove pb only.

https://jira.sw.ru:9443/browse/PSBM-6799
---
 include/ub/ub_page.h   |    2 +-
 kernel/slm/if26.c      |   12 ++++--
 kernel/ub/ub_page_bc.c |  104 +++++++++++++++++++++---------------------------
 3 files changed, 55 insertions(+), 63 deletions(-)

diff --git a/include/ub/ub_page.h b/include/ub/ub_page.h
index 1f7e789..7e223a0 100644
--- a/include/ub/ub_page.h
+++ b/include/ub/ub_page.h
@@ -54,6 +54,6 @@ struct address_space;
 extern int is_shmem_mapping(struct address_space *);
 
 struct mm_struct;
-void ub_migrate_mm(struct mm_struct *, struct user_beancounter *);
+int ub_migrate_mm(struct mm_struct *, struct user_beancounter *);
 
 #endif
diff --git a/kernel/ub/ub_page_bc.c b/kernel/ub/ub_page_bc.c
index 417bfb1..67abd0f 100644
--- a/kernel/ub/ub_page_bc.c
+++ b/kernel/ub/ub_page_bc.c
@@ -390,13 +390,11 @@ static void __pb_add_ref(struct page *page, struct user_beancounter *bc,
 	inc_held_pages(bc, UB_PAGE_WEIGHT >> shift);
 }
 
-void pb_add_ref(struct page *page, struct mm_struct *mm,
+static void pb_add_ref_ub(struct page *page, struct user_beancounter *bc,
 		struct page_beancounter **p_pb)
 {
 	int hash;
-	struct user_beancounter *bc;
 
-	bc = mm->mm_ub;
 	if (bc == NULL)
 		return;
 
@@ -411,6 +409,14 @@ void pb_add_ref(struct page *page, struct mm_struct *mm,
 	spin_unlock(&pb_hash_table[hash].lock);
 }
 
+void pb_add_ref(struct page *page, struct mm_struct *mm,
+		struct page_beancounter **p_pb)
+{
+	struct user_beancounter *bc;
+	bc = mm->mm_ub;
+	pb_add_ref_ub(page, bc, p_pb);	
+}
+
 void pb_dup_ref(struct page *page, struct mm_struct *mm,
 		struct page_beancounter **p_pb)
 {
@@ -446,15 +452,13 @@ void pb_dup_ref(struct page *page, struct mm_struct *mm,
 	}
 }
 
-void pb_remove_ref(struct page *page, struct mm_struct *mm)
+static void pb_remove_ref_ub(struct page *page, struct user_beancounter *bc)
 {
 	int hash;
-	struct user_beancounter *bc;
 	struct page_beancounter *p, **q, *f;
 	spinlock_t *pl;
 	int shift, shiftt;
 
-	bc = mm->mm_ub;
 	if (bc == NULL)
 		return;
 
@@ -555,6 +559,13 @@ out_rcu:
 	rcu_read_unlock();
 }
 
+void pb_remove_ref(struct page *page, struct mm_struct *mm)
+{
+	struct user_beancounter *bc;
+	bc = mm->mm_ub;
+	pb_remove_ref_ub(page, bc);
+}
+
 struct user_beancounter *pb_grab_page_ub(struct page *page)
 {
 	struct page_beancounter *pb;
@@ -595,52 +606,11 @@ void __init ub_init_pbc(void)
 		spin_lock_init(&page_locks[i]);
 }
 
-static void ub_migrate_page(struct page *page,
-		struct user_beancounter *old_ub,
-		struct user_beancounter *new_ub)
-{
-	unsigned hash1, hash2;
-	struct page_beancounter *pb, **prev, **head;
-
-	/* implemented only for single-mmaped pages */
-	BUG_ON(page_mapcount(page) != 1);
-
-	hash1 = pb_hash(old_ub, page);
-	hash2 = pb_hash(new_ub, page);
-	prev = &pb_hash_table[hash1].first;
-	head = &pb_hash_table[hash2].first;
-
-	if (new_ub->parent != old_ub)
-		do_dec_held_pages(old_ub, UB_PAGE_WEIGHT);
-	do_inc_held_pages(new_ub, UB_PAGE_WEIGHT);
-
-	if (new_ub->parent != old_ub)
-		ub_stat_dec(old_ub, pbcs);
-	ub_stat_inc(new_ub, pbcs);
-
-	pb = page_pbc(page);
-
-	BUG_ON(pb->ub != old_ub);
-
-	spin_lock(&pb_hash_table[hash1].lock);
-	while (*prev != pb)
-		prev = &(*prev)->next_hash;
-	*prev = pb->next_hash;
-	spin_unlock(&pb_hash_table[hash1].lock);
-
-	spin_lock(&pb_hash_table[hash2].lock);
-	pb->next_hash = *head;
-	*head = pb;
-	spin_unlock(&pb_hash_table[hash2].lock);
-
-	pb->ub = get_beancounter_fast(new_ub);
-	put_beancounter_fast(old_ub);
-}
-
 static inline void ub_migrate_pte_range(struct vm_area_struct *vma,
 		pmd_t *pmd, unsigned long addr, unsigned long end,
 		struct user_beancounter *old_ub,
-		struct user_beancounter *new_ub)
+		struct user_beancounter *new_ub,
+		struct page_beancounter **p_pb)
 {
 	pte_t *pte;
 	spinlock_t *ptl;
@@ -653,7 +623,11 @@ static inline void ub_migrate_pte_range(struct vm_area_struct *vma,
 		page = vm_normal_page(vma, addr, *pte);
 		if (!page)
 			continue;
-		ub_migrate_page(page, old_ub, new_ub);
+		/* implemented only for single-mmaped pages */
+		BUG_ON(page_mapcount(page) != 1);
+
+		pb_remove_ref_ub(page, old_ub);
+		pb_add_ref_ub(page, new_ub, p_pb);
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	pte_unmap_unlock(pte - 1, ptl);
 }
@@ -661,7 +637,8 @@ static inline void ub_migrate_pte_range(struct vm_area_struct *vma,
 static inline void ub_migrate_pmd_range(struct vm_area_struct *vma,
 		pud_t *pud, unsigned long addr, unsigned long end,
 		struct user_beancounter *old_ub,
-		struct user_beancounter *new_ub)
+		struct user_beancounter *new_ub,
+		struct page_beancounter **p_pb)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -672,14 +649,15 @@ static inline void ub_migrate_pmd_range(struct vm_area_struct *vma,
 		if (pmd_none_or_clear_bad(pmd))
 			continue;
 		ub_migrate_pte_range(vma, pmd, addr, next,
-				old_ub, new_ub);
+				old_ub, new_ub, p_pb);
 	} while (pmd++, addr = next, addr != end);
 }
 
 static inline void ub_migrate_pud_range(struct vm_area_struct *vma,
 		pgd_t *pgd, unsigned long addr, unsigned long end,
 		struct user_beancounter *old_ub,
-		struct user_beancounter *new_ub)
+		struct user_beancounter *new_ub,
+		struct page_beancounter **p_pb)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -690,14 +668,15 @@ static inline void ub_migrate_pud_range(struct vm_area_struct *vma,
 		if (pud_none_or_clear_bad(pud))
 			continue;
 		ub_migrate_pmd_range(vma, pud, addr, next,
-				old_ub, new_ub);
+				old_ub, new_ub, p_pb);
 	} while (pud++, addr = next, addr != end);
 }
 
 static void ub_migrate_vma_range(struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end,
 		struct user_beancounter *old_ub,
-		struct user_beancounter *new_ub)
+		struct user_beancounter *new_ub,
+		struct page_beancounter **p_pb)
 {
 	pgd_t *pgd;
 	unsigned long next;
@@ -708,15 +687,22 @@ static void ub_migrate_vma_range(struct vm_area_struct *vma,
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
 		ub_migrate_pud_range(vma, pgd, addr, next,
-				old_ub, new_ub);
+				old_ub, new_ub, p_pb);
 	} while (pgd++, addr = next, addr != end);
 }
 
-void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
+int ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 {
 	struct user_beancounter *old_ub = mm->mm_ub;
 	struct vm_area_struct *vma;
 	unsigned long size, nr_vmas = 0;
+	struct page_beancounter *p_pb = NULL;
+	unsigned long rss = get_mm_rss(mm);
+	int ret;
+
+	ret = pb_alloc_list(&p_pb, rss);
+	if (ret)
+		return ret;
 
 	/* implemented only migration into sub-beancounter */
 	BUG_ON(new_ub->parent != top_beancounter(old_ub));
@@ -749,7 +735,7 @@ void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 		}
 
 		ub_migrate_vma_range(vma, vma->vm_start, vma->vm_end,
-				old_ub, new_ub);
+				old_ub, new_ub, &p_pb);
 	}
 
 	size = mm->page_table_charged << PAGE_SHIFT;
@@ -762,5 +748,7 @@ void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 	put_beancounter(old_ub);
 
 	up_write(&mm->mmap_sem);
+	pb_free_list(&p_pb);
+	return 0;
 }
 EXPORT_SYMBOL(ub_migrate_mm);
-- 1.5.5.6 . 
