From: Paolo Bonzini <pbonzini@redhat.com>
Date: Mon, 30 Aug 2010 16:46:28 -0400
Subject: [xen] virtual NMI support
Message-id: <1283186789-20909-4-git-send-email-pbonzini@redhat.com>
Patchwork-id: 27935
O-Subject: [RHEL5.6 XEN PATCH 3/4] virtual NMI support
Bugzilla: 625902

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=625902

Upstream status: http://xenbits.xensource.com/xen-unstable.hg/rev/15466
    http://xenbits.xensource.com/xen-unstable.hg/rev/15467

Brew build: https://brewweb.devel.redhat.com/taskinfo?taskID=2719051

Included here to ease backport of task switching.  Can be disabled
if necessary by undoing the vmcs.c hunk.
---
 arch/x86/hvm/vmx/intr.c        |   63 +++++++++++++++++++++++++++++----------
 arch/x86/hvm/vmx/vmcs.c        |    2 +-
 arch/x86/hvm/vmx/vmx.c         |   21 +++++++++++--
 include/asm-x86/hvm/vmx/vmcs.h |    2 +
 include/asm-x86/hvm/vmx/vmx.h  |    2 +
 5 files changed, 69 insertions(+), 21 deletions(-)

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/hvm/vmx/intr.c b/arch/x86/hvm/vmx/intr.c
index 21f5a49..b29e978 100644
--- a/arch/x86/hvm/vmx/intr.c
+++ b/arch/x86/hvm/vmx/intr.c
@@ -71,13 +71,39 @@
  * the effect is cleared. (i.e., MOV-SS-blocking 'dominates' STI-blocking).
  */
 
-static void enable_irq_window(struct vcpu *v)
+static void enable_intr_window(struct vcpu *v, enum hvm_intack intr_source)
 {
-    u32  *cpu_exec_control = &v->arch.hvm_vmx.exec_control;
-    
-    if ( !(*cpu_exec_control & CPU_BASED_VIRTUAL_INTR_PENDING) )
+    u32 *cpu_exec_control = &v->arch.hvm_vcpu.u.vmx.exec_control;
+    u32 ctl = CPU_BASED_VIRTUAL_INTR_PENDING;
+
+    if ( unlikely(intr_source == hvm_intack_none) )
+        return;
+
+    if ( unlikely(intr_source == hvm_intack_nmi) && cpu_has_vmx_vnmi )
+    {
+        /*
+         * We set MOV-SS blocking in lieu of STI blocking when delivering an
+         * NMI. This is because it is processor-specific whether STI-blocking
+         * blocks NMIs. Hence we *must* check for STI-blocking on NMI delivery
+         * (otherwise vmentry will fail on processors that check for STI-
+         * blocking) but if the processor does not check for STI-blocking then
+         * we may immediately vmexit and hance make no progress!
+         * (see SDM 3B 21.3, "Other Causes of VM Exits").
+         */
+        u32 intr_shadow = __vmread(GUEST_INTERRUPTIBILITY_INFO);
+        if ( intr_shadow & VMX_INTR_SHADOW_STI )
+        {
+            /* Having both STI-blocking and MOV-SS-blocking fails vmentry. */
+            intr_shadow &= ~VMX_INTR_SHADOW_STI;
+            intr_shadow |= VMX_INTR_SHADOW_MOV_SS;
+            __vmwrite(GUEST_INTERRUPTIBILITY_INFO, intr_shadow);
+        }
+        ctl = CPU_BASED_VIRTUAL_NMI_PENDING;
+    }
+
+    if ( !(*cpu_exec_control & ctl) )
     {
-        *cpu_exec_control |= CPU_BASED_VIRTUAL_INTR_PENDING;
+        *cpu_exec_control |= ctl;
         __vmwrite(CPU_BASED_VM_EXEC_CONTROL, *cpu_exec_control);
     }
 }
@@ -124,8 +150,7 @@ asmlinkage void vmx_intr_assist(void)
         if ( unlikely(v->arch.hvm_vmx.vector_injected) )
         {
             v->arch.hvm_vmx.vector_injected = 0;
-            if ( unlikely(intr_source != hvm_intack_none) )
-                enable_irq_window(v);
+            enable_intr_window(v, intr_source);
             return;
         }
 
@@ -133,7 +158,9 @@ asmlinkage void vmx_intr_assist(void)
         idtv_info_field = __vmread(IDT_VECTORING_INFO_FIELD);
         if ( unlikely(idtv_info_field & INTR_INFO_VALID_MASK) )
         {
-            __vmwrite(VM_ENTRY_INTR_INFO_FIELD, idtv_info_field);
+            /* See SDM 3B 25.7.1.1 and .2 for info about masking resvd bits. */
+            __vmwrite(VM_ENTRY_INTR_INFO_FIELD,
+                      idtv_info_field & ~INTR_INFO_RESVD_BITS_MASK);
 
             /*
              * Safe: the length will only be interpreted for software
@@ -147,8 +174,17 @@ asmlinkage void vmx_intr_assist(void)
             if ( unlikely(idtv_info_field & 0x800) ) /* valid error code */
                 __vmwrite(VM_ENTRY_EXCEPTION_ERROR_CODE,
                           __vmread(IDT_VECTORING_ERROR_CODE));
-            if ( unlikely(intr_source != hvm_intack_none) )
-                enable_irq_window(v);
+
+            /*
+             * Clear NMI-blocking interruptibility info if an NMI delivery
+             * faulted. Re-delivery will re-set it (see SDM 3B 25.7.1.2).
+             */
+            if ( (idtv_info_field&INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI )
+                __vmwrite(GUEST_INTERRUPTIBILITY_INFO,
+                          __vmread(GUEST_INTERRUPTIBILITY_INFO) &
+                          ~VMX_INTR_SHADOW_NMI);
+
+            enable_intr_window(v, intr_source);
 
             HVM_DBG_LOG(DBG_LEVEL_1, "idtv_info_field=%x", idtv_info_field);
             return;
@@ -157,14 +193,9 @@ asmlinkage void vmx_intr_assist(void)
         if ( likely(intr_source == hvm_intack_none) )
             return;
 
-        /*
-         * TODO: Better NMI handling. Shouldn't wait for EFLAGS.IF==1, but
-         * should wait for exit from 'NMI blocking' window (NMI injection to
-         * next IRET). This requires us to use the new 'virtual NMI' support.
-         */
         if ( !hvm_interrupts_enabled(v, intr_source) )
         {
-            enable_irq_window(v);
+            enable_intr_window(v, intr_source);
             return;
         }
     } while ( !hvm_vcpu_ack_pending_irq(v, intr_source, &intr_vector) );
diff --git a/arch/x86/hvm/vmx/vmcs.c b/arch/x86/hvm/vmx/vmcs.c
index 015e2be..895084d 100644
--- a/arch/x86/hvm/vmx/vmcs.c
+++ b/arch/x86/hvm/vmx/vmcs.c
@@ -82,7 +82,7 @@ void vmx_init_vmcs_config(void)
 
     min = (PIN_BASED_EXT_INTR_MASK |
            PIN_BASED_NMI_EXITING);
-    opt = 0; /*PIN_BASED_VIRTUAL_NMIS*/
+    opt = PIN_BASED_VIRTUAL_NMIS;
     _vmx_pin_based_exec_control = adjust_vmx_controls(
         min, opt, MSR_IA32_VMX_PINBASED_CTLS_MSR);
 
diff --git a/arch/x86/hvm/vmx/vmx.c b/arch/x86/hvm/vmx/vmx.c
index 1f6d63d..9e27dbd 100644
--- a/arch/x86/hvm/vmx/vmx.c
+++ b/arch/x86/hvm/vmx/vmx.c
@@ -1262,15 +1262,17 @@ static int vmx_interrupts_enabled(struct vcpu *v, enum hvm_intack type)
 
     ASSERT(v == current);
 
-    intr_shadow  = __vmread(GUEST_INTERRUPTIBILITY_INFO);
-    intr_shadow &= VMX_INTR_SHADOW_STI|VMX_INTR_SHADOW_MOV_SS;
+    intr_shadow = __vmread(GUEST_INTERRUPTIBILITY_INFO);
 
     if ( type == hvm_intack_nmi )
-        return !intr_shadow;
+        return !(intr_shadow & (VMX_INTR_SHADOW_STI|
+                                VMX_INTR_SHADOW_MOV_SS|
+                                VMX_INTR_SHADOW_NMI));
 
     ASSERT((type == hvm_intack_pic) || (type == hvm_intack_lapic));
     eflags = __vmread(GUEST_RFLAGS);
-    return (eflags & X86_EFLAGS_IF) && !intr_shadow;
+    return ((eflags & X86_EFLAGS_IF) &&
+            !(intr_shadow & (VMX_INTR_SHADOW_STI|VMX_INTR_SHADOW_MOV_SS)));
 }
 
 
@@ -3254,6 +3256,17 @@ asmlinkage void vmx_vmexit_handler(struct cpu_user_regs *regs)
 
         vector = intr_info & INTR_INFO_VECTOR_MASK;
 
+        /*
+         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B
+         * 25.7.1.2, "Resuming Guest Software after Handling an Exception").
+         * (NB. If we emulate this IRET for any reason, we should re-clear!)
+         */
+        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&
+             !(__vmread(IDT_VECTORING_INFO_FIELD) & INTR_INFO_VALID_MASK) &&
+             (vector != TRAP_double_fault) )
+            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,
+                    __vmread(GUEST_INTERRUPTIBILITY_INFO)|VMX_INTR_SHADOW_NMI);
+
         perfc_incra(cause_vector, vector);
 
         switch ( vector )
diff --git a/include/asm-x86/hvm/vmx/vmcs.h b/include/asm-x86/hvm/vmx/vmcs.h
index 029c7a9..16887b7 100644
--- a/include/asm-x86/hvm/vmx/vmcs.h
+++ b/include/asm-x86/hvm/vmx/vmcs.h
@@ -159,6 +159,8 @@ extern u32 vmx_secondary_exec_control;
     (vmx_secondary_exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)
 #define cpu_has_vmx_tpr_shadow \
     (vmx_cpu_based_exec_control & CPU_BASED_TPR_SHADOW)
+#define cpu_has_vmx_vnmi \
+    (vmx_pin_based_exec_control & PIN_BASED_VIRTUAL_NMIS)
 #define cpu_has_vmx_mmap_vtpr_optimization \
     (cpu_has_vmx_virtualize_apic_accesses && cpu_has_vmx_tpr_shadow)
 
diff --git a/include/asm-x86/hvm/vmx/vmx.h b/include/asm-x86/hvm/vmx/vmx.h
index c701a7f..587e1f3 100644
--- a/include/asm-x86/hvm/vmx/vmx.h
+++ b/include/asm-x86/hvm/vmx/vmx.h
@@ -117,7 +117,9 @@ extern struct page_info *change_guest_physmap_for_vtpr(struct domain *d,
 #define INTR_INFO_VECTOR_MASK           0xff            /* 7:0 */
 #define INTR_INFO_INTR_TYPE_MASK        0x700           /* 10:8 */
 #define INTR_INFO_DELIVER_CODE_MASK     0x800           /* 11 */
+#define INTR_INFO_NMI_UNBLOCKED_BY_IRET 0x1000          /* 12 */
 #define INTR_INFO_VALID_MASK            0x80000000      /* 31 */
+#define INTR_INFO_RESVD_BITS_MASK       0x7ffff000
 
 #define INTR_TYPE_EXT_INTR              (0 << 8)    /* external interrupt */
 #define INTR_TYPE_NMI                   (2 << 8)    /* NMI                */
