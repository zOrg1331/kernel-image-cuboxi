Subject: [PATCH rh5 2/3] ubc: recharge vma in ub_migrate_mm
From: Konstantin Khlebnikov <khlebnikov@parallels.com>
Date: Wed, 9 Feb 2011 14:05:33 +0300
To: "vzlin-dev@sw.ru" <vzlin-dev@sw.ru>
CC: Pavel Emelianov <xemul@parallels.com>

Recharge vm_area_struct at mm migration into/between sub-beancointers

https://jira.sw.ru/browse/PSBM-6788

Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
---
 include/linux/slab.h   |    1 +
 kernel/fork.c          |    2 +-
 kernel/ub/ub_page_bc.c |    6 +++++-
 3 files changed, 7 insertions(+), 2 deletions(-)

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 678dddc..160ea19 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -309,6 +309,7 @@ extern struct vm_area_struct *allocate_vma(struct mm_struct *mm, gfp_t gfp_flags
 extern void free_vma(struct mm_struct *mm, struct vm_area_struct *vma);
 
 /* System wide caches */
+extern kmem_cache_t	*__vm_area_cachep;
 extern kmem_cache_t	*names_cachep;
 extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;
diff --git a/kernel/fork.c b/kernel/fork.c
index afafc3b..95b4915 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -129,7 +129,7 @@ kmem_cache_t *files_cachep;
 kmem_cache_t *fs_cachep;
 
 /* SLAB cache for vm_area_struct structures */
-static kmem_cache_t *__vm_area_cachep;
+kmem_cache_t *__vm_area_cachep;
 
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static kmem_cache_t *mm_cachep;
diff --git a/kernel/ub/ub_page_bc.c b/kernel/ub/ub_page_bc.c
index 1238a43..1dbb0cd 100644
--- a/kernel/ub/ub_page_bc.c
+++ b/kernel/ub/ub_page_bc.c
@@ -15,11 +15,13 @@
 #include <linux/vmalloc.h>
 #include <linux/module.h>
 #include <linux/highmem.h>
+#include <linux/kmem_cache.h>
 
 #include <ub/beancounter.h>
 #include <ub/ub_hash.h>
 #include <ub/ub_vmpages.h>
 #include <ub/ub_page.h>
+#include <ub/ub_mem.h>
 #include <ub/io_acct.h>
 
 #define page_pblist(page)      (&page_pbc(page))
@@ -713,7 +715,7 @@ void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 {
 	struct user_beancounter *old_ub = mm->mm_ub;
 	struct vm_area_struct *vma;
-	unsigned long size;
+	unsigned long size, nr_vmas = 0;
 
 	/* implemented only migration into sub-beancounter */
 	BUG_ON(new_ub->parent != top_beancounter(old_ub));
@@ -721,6 +723,7 @@ void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 	down_write(&mm->mmap_sem);
 
 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
+		nr_vmas++;
 		/* UB_PRIVVMPAGES charged only on top_beancounter */
 		if (vma->vm_flags & VM_LOCKED) {
 			unsigned long size;
@@ -749,6 +752,7 @@ void ub_migrate_mm(struct mm_struct *mm, struct user_beancounter *new_ub)
 	}
 
 	size = mm->page_table_charged << PAGE_SHIFT;
+	size += nr_vmas * CHARGE_SIZE(__vm_area_cachep->objuse);
 
 	uncharge_beancounter_notop(old_ub, UB_KMEMSIZE, size);
 	charge_beancounter_notop(new_ub, UB_KMEMSIZE, size);

