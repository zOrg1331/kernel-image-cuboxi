From: Peter Zijlstra <pzijlstr@redhat.com>
Date: Tue, 11 Dec 2007 19:04:42 +0100
Subject: [sched] fair scheduler
Message-id: 1197396282.7509.5.camel@taijtu
O-Subject: [RHEL5.2 PATCH] BZ250589:
Bugzilla: 250589

BZ: https://bugzilla.redhat.com/show_bug.cgi?id=250589

Provide a sysctl to switch off the scheduler's interactivity estimator,
because a customer is hitting the wrong scheduling patterns and finds
that it significantly hurts their performance.

Keep the interactivity estimator enabled by default so that the
scheduler does not differ from previous RHEL-5.x releases.

Acked-by: Pete Zaitcev <zaitcev@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b69d4a2..fd7059f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -118,6 +118,7 @@ extern unsigned long avenrun[];		/* Load averages */
 	load += n*(FIXED_1-exp); \
 	load >>= FSHIFT;
 
+extern unsigned int sched_fair;
 extern unsigned long total_forks;
 extern int nr_threads;
 extern int last_pid;
diff --git a/include/linux/sysctl.h b/include/linux/sysctl.h
index 0748a18..b5d7351 100644
--- a/include/linux/sysctl.h
+++ b/include/linux/sysctl.h
@@ -155,6 +155,7 @@ enum
 	KERN_MAX_LOCK_DEPTH=74,
 	KERN_KDUMP_ON_INIT=75,	/* int: ia64 kdump with INIT */
  	KERN_PANIC_ON_NMI=76, /* int: whether we will panic on an unrecovered */
+	KERN_SCHED_FAIR=77,     /* int: disable the interactivity estimator */
 	KERN_SOFTLOCKUP_THRESH=80, /* int: min time to report softlockups */
 };
 
diff --git a/kernel/sched.c b/kernel/sched.c
index 66e4fe5..90d03bb 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -867,7 +867,7 @@ static int recalc_task_prio(struct task_struct *p, unsigned long long now)
 	/* Caller must always ensure 'now >= p->timestamp' */
 	unsigned long sleep_time = now - p->timestamp;
 
-	if (batch_task(p))
+	if (batch_task(p) || sched_fair)
 		sleep_time = 0;
 
 	if (likely(sleep_time > 0)) {
@@ -4059,6 +4059,8 @@ static inline struct task_struct *find_process_by_pid(pid_t pid)
 	return pid ? find_task_by_pid(pid) : current;
 }
 
+unsigned int sched_fair;
+
 /* Actually do priority change: must hold rq lock. */
 static void __setscheduler(struct task_struct *p, int policy, int prio)
 {
@@ -4074,6 +4076,8 @@ static void __setscheduler(struct task_struct *p, int policy, int prio)
 	 */
 	if (policy == SCHED_BATCH)
 		p->sleep_avg = 0;
+	if (policy == SCHED_NORMAL && sched_fair)
+		p->sleep_avg = 0;
 	set_load_weight(p);
 }
 
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 9a1bcc5..79eae6e 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -263,6 +263,14 @@ static ctl_table root_table[] = {
 
 static ctl_table kern_table[] = {
 	{
+		.ctl_name	= KERN_SCHED_FAIR,
+		.procname	= "sched_fair",
+		.data		= &sched_fair,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
 		.ctl_name	= KERN_OSTYPE,
 		.procname	= "ostype",
 		.data		= system_utsname.sysname,
