From: Don Zickus <dzickus@redhat.com>
Date: Tue, 9 Jun 2009 12:04:43 -0400
Subject: Revert: [net] avoid extra wakeups in wait_for_packet
Message-id: 20090609160443.GU25818@redhat.com
O-Subject: Re: [RHEL 5.4 PATCH]: net: avoid extra wakeups in wait_for_packet()
Bugzilla: 497897

On Wed, May 20, 2009 at 02:15:51PM -0400, Neil Horman wrote:
> Hey all-
> 	This is a backport of upstream commit
> bf368e4e70cd4e0f880923c44e95a4273d725ab4
> It improves performance by avoiding the needless waking up of threads blocked in
> wait_for_packet.  Resolves bz 497897

QE has noticed this patch is causing network performance degradation on
s390x boxes with ftp and http installs.  Seeing as this is not a critical
patch, I will be pulling it for the next build tomorrow night.

Cheers,
Don

diff --git a/include/linux/wait.h b/include/linux/wait.h
index 6014bda..2e079bf 100644
--- a/include/linux/wait.h
+++ b/include/linux/wait.h
@@ -142,6 +142,8 @@ static inline void __remove_wait_queue(wait_queue_head_t *head,
 }
 
 void FASTCALL(__wake_up(wait_queue_head_t *q, unsigned int mode, int nr, void *key));
+extern void FASTCALL(__wake_up_locked(wait_queue_head_t *q, unsigned int mode));
+extern void FASTCALL(__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr));
 void FASTCALL(__wake_up_bit(wait_queue_head_t *, void *, int));
 int FASTCALL(__wait_on_bit(wait_queue_head_t *, struct wait_bit_queue *, int (*)(void *), unsigned));
 int FASTCALL(__wait_on_bit_lock(wait_queue_head_t *, struct wait_bit_queue *, int (*)(void *), unsigned));
@@ -149,11 +151,6 @@ void FASTCALL(wake_up_bit(void *, int));
 int FASTCALL(out_of_line_wait_on_bit(void *, int, int (*)(void *), unsigned));
 int FASTCALL(out_of_line_wait_on_bit_lock(void *, int, int (*)(void *), unsigned));
 wait_queue_head_t *FASTCALL(bit_waitqueue(void *, int));
-void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key);
-void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode, int nr,
-			void *key);
-void __wake_up_locked(wait_queue_head_t *q, unsigned int mode);
-void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);
 
 #define wake_up(x)			__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, NULL)
 #define wake_up_nr(x, nr)		__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, nr, NULL)
@@ -164,15 +161,6 @@ void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);
 #define	wake_up_locked(x)		__wake_up_locked((x), TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE)
 #define wake_up_interruptible_sync(x)   __wake_up_sync((x),TASK_INTERRUPTIBLE, 1)
 
-#define wake_up_poll(x, m)                            \
-	__wake_up(x, TASK_NORMAL, 1, (void *) (m))
-#define wake_up_locked_poll(x, m)                             \
-	__wake_up_locked_key((x), TASK_NORMAL, (void *) (m))
-#define wake_up_interruptible_poll(x, m)                      \
-	__wake_up(x, TASK_INTERRUPTIBLE, 1, (void *) (m))
-#define wake_up_interruptible_sync_poll(x, m)                         \
-	__wake_up_sync_key((x), TASK_INTERRUPTIBLE, 1, (void *) (m))
-
 #define __wait_event(wq, condition) 					\
 do {									\
 	DEFINE_WAIT(__wait);						\
diff --git a/kernel/sched.c b/kernel/sched.c
index fa71407..9921513 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -3823,17 +3823,11 @@ void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
 	__wake_up_common(q, mode, 1, 0, NULL);
 }
 
-void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)
-{
-	__wake_up_common(q, mode, 1, 0, key);
-}
-
 /**
- * __wake_up_sync_key - wake up threads blocked on a waitqueue.
+ * __wake_up_sync - wake up threads blocked on a waitqueue.
  * @q: the waitqueue
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
- * @key: opaque value to be passed to wakeup targets
  *
  * The sync wakeup differs that the waker knows that it will schedule
  * away soon, so while the target thread will be woken up, it will not
@@ -3842,8 +3836,8 @@ void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)
  *
  * On UP it can prevent extra preemption.
  */
-void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,
-			int nr_exclusive, void *key)
+void fastcall
+__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 {
 	unsigned long flags;
 	int sync = 1;
@@ -3855,18 +3849,9 @@ void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,
 		sync = 0;
 
 	spin_lock_irqsave(&q->lock, flags);
-	__wake_up_common(q, mode, nr_exclusive, sync, key);
+	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL_GPL(__wake_up_sync_key);
-
-/*
- * __wake_up_sync - see __wake_up_sync_key()
- */
-void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
-{
-	__wake_up_sync_key(q, mode, nr_exclusive, NULL);
-}
 EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
 
 void fastcall complete(struct completion *x)
diff --git a/net/core/sock.c b/net/core/sock.c
index 909085d..4bb1732 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -1521,7 +1521,7 @@ static void sock_def_error_report(struct sock *sk)
 {
 	read_lock(&sk->sk_callback_lock);
 	if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
-		wake_up_interruptible_poll(sk->sk_sleep, POLLERR);
+		wake_up_interruptible(sk->sk_sleep);
 	sk_wake_async(sk,0,POLL_ERR); 
 	read_unlock(&sk->sk_callback_lock);
 }
@@ -1530,8 +1530,7 @@ static void sock_def_readable(struct sock *sk, int len)
 {
 	read_lock(&sk->sk_callback_lock);
 	if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
-		wake_up_interruptible_sync_poll(sk->sk_sleep, POLLIN |
-					POLLRDNORM | POLLRDBAND);
+		wake_up_interruptible(sk->sk_sleep);
 	sk_wake_async(sk,1,POLL_IN);
 	read_unlock(&sk->sk_callback_lock);
 }
@@ -1545,8 +1544,8 @@ static void sock_def_write_space(struct sock *sk)
 	 */
 	if((atomic_read(&sk->sk_wmem_alloc) << 1) <= sk->sk_sndbuf) {
 		if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
-			wake_up_interruptible_sync_poll(sk->sk_sleep, POLLOUT |
-					POLLWRNORM | POLLWRBAND);
+			wake_up_interruptible(sk->sk_sleep);
+
 		/* Should agree with poll, otherwise some programs break */
 		if (sock_writeable(sk))
 			sk_wake_async(sk, 2, POLL_OUT);
